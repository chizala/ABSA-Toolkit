{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # Using opensource python libraries\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "from nltk import tokenize\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "tagger = PerceptronTagger()\n",
    "import joblib\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "def load_lexicon(lex_type):\n",
    "    lex = []\n",
    "\n",
    "    f = open(lex_type+\"_lexicon.txt\", \"r\")\n",
    "    for line in f:\n",
    "        tag = line.split()[0]\n",
    "        lex.append(tag)\n",
    "        \n",
    "    return lex\n",
    "\n",
    "#LOADING MODELS\n",
    "vectors_filename = \"vectors_yelp_200.txt\" #user provided\n",
    "lex_file = 'saif_lex/Yelp-restaurant-reviews-AFFLEX-NEGLEX-unigrams.txt'\n",
    "pos_lexicon = load_lexicon(\"lexica/restaurants/ote/pos\")\n",
    "        \n",
    "model = gensim.models.Word2Vec.load_word2vec_format(vectors_filename,binary=False)\n",
    "ndim = model.vector_size\n",
    "filename = 'models/ote/otemodel.sav'\n",
    "ssvm = pickle.load(open(filename, 'rb'))\n",
    "index2word_set = set(model.index2word)\n",
    "\n",
    "#LOADING LEXICON FOR OTE TASK\n",
    "\n",
    "    \n",
    "#LOADING LEXICONS FOR POLARITY DETECTION TASK\n",
    "def load_polarity_lexicon(filename):\n",
    "    lex = []\n",
    "    scores=[]\n",
    "    f = open(filename, \"r\")#'saif_lex/Yelp-restaurant-reviews-AFFLEX-NEGLEX-unigrams.txt'\n",
    "    for line in f:\n",
    "        l = line.split()\n",
    "        tag = l[0]\n",
    "        score = l[1]\n",
    "        if(len(re.findall('[_NEG]',tag)) == 0):\n",
    "            lex.append(tag)\n",
    "            scores.append(score)\n",
    "        \n",
    "    return lex,scores\n",
    "# In[20]:\n",
    "\n",
    "yelp_lex,yelp_scores = load_polarity_lexicon(lex_file)\n",
    "directory = 'models/acd/'\n",
    "categories = []\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.sav'):\n",
    "                       s = str(file.lower())\n",
    "                       result = re.search('model_(.*).sav', str(s))\n",
    "                       categories.append(result.group(1))\n",
    "models_acd = []\n",
    "for c in categories:\n",
    "          loaded_model = pickle.load(open('models/acd/model_'+ str(c)+'.sav', 'rb'))\n",
    "          models_acd.append(loaded_model)\n",
    "\n",
    "#POLARITY DETECTION MODELS\n",
    "# Need to make it dynamic\n",
    "models_pd = []\n",
    "polarities = []\n",
    "directory_pd = 'models/pd/'\n",
    "for root, dirs, files in os.walk(directory_pd):\n",
    "    for file in files:\n",
    "        if file.endswith('.sav'):\n",
    "                       s = str(file.lower())\n",
    "                       result = re.search('model_(.*).sav', str(s))\n",
    "                       polarities.append(result.group(1))\n",
    "\n",
    "for c in polarities:\n",
    "          loaded_model = pickle.load(open('models/pd/model_'+ str(c)+'.sav', 'rb'))\n",
    "          models_pd.append(loaded_model)\n",
    "\n",
    "#model_neutral = pickle.load(open('models/pd/model_neutral.sav', 'rb'))\n",
    "#model_positive = pickle.load(open('models/pd/model_positive.sav', 'rb'))\n",
    "#model_negative = pickle.load(open('models/pd/model_negative.sav', 'rb'))\n",
    "\n",
    "\n",
    "def create_lexicon_feats(x,yelp_lex,yelp_scores):\n",
    "    x = x.lower()\n",
    "    x = re.sub('\\.',' ',x).strip()\n",
    "    words = x.split(' ')\n",
    "    count = 0\n",
    "    for j in range(0,len(words)):\n",
    "        if (words[j] not in ['don\\'t','no','dont','not','never']):\n",
    "            for i in range(0,len(yelp_lex)):\n",
    "                \n",
    "                 if words[j] == yelp_lex[i]:\n",
    "                    if(j >0):\n",
    "                        if(words[j-1] in ['don\\'t','no','dont','not','never']):\n",
    "                            count = count + (-1)*float(yelp_scores[i])\n",
    "                        else:\n",
    "                            count = count + float(yelp_scores[i])\n",
    "                    else:        \n",
    "                        count = count + float(yelp_scores[i])\n",
    "                    break\n",
    "                    \n",
    "       \n",
    "    return count\n",
    "       \n",
    "\n",
    "# In[22]:\n",
    "# need to make it dynamic\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "stopwords =  nltk.corpus.stopwords.words('english')\n",
    "not_remove = ['not','very','few','more','only','nor','but','don','t','and','with','no']\n",
    "for word in not_remove:\n",
    "    stopwords.remove(word)\n",
    "    \n",
    "#for polarity and category detection task\n",
    "def review_to_words( raw_review ):\n",
    "    #remove non alphanumeric characters\n",
    "    letters_only = re.sub(\"\\'\", \"\",raw_review) \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",letters_only) \n",
    "    # convert into lowercase and split text into words using split() function\n",
    "    words = letters_only.lower().split()\n",
    "    # declaring empty array\n",
    "    cleanwords = []\n",
    "    for word in words:\n",
    "        if(word not in stopwords):\n",
    "            cleanwords.append(word)\n",
    "    return( \" \".join( cleanwords ))\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=True ):\n",
    "    # remove HTML\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "         words = [w for w in words if not w in stopwords]\n",
    "            \n",
    "    return ( words )\n",
    "\n",
    "\n",
    "#for ote task\n",
    "def review_to_words_ote( raw_review ):\n",
    "    #remove non alphanumeric characters\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \",raw_review) \n",
    "    # convert into lowercase and split text into words using split() function\n",
    "    words = letters_only.split()\n",
    "    # declaring empty array\n",
    "    cleanwords = []\n",
    "    for word in words: #if(word.lower() not in stopwords and len(word) > 2):\n",
    "       \n",
    "        cleanwords.append(word)\n",
    "    return( \" \".join( cleanwords ))\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "# functions defined below are for ote feature extraction\n",
    "\n",
    "def create_vector_features(x,model):\n",
    "    ndim = model.vector_size\n",
    "    index2word_set = set( model.index2word )\n",
    "    words = x.lower().split(\" \")\n",
    "    vec_feats =[]\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            vec_feats.append(model[w])\n",
    "        else:\n",
    "            vec_feats.append(np.zeros(ndim))\n",
    "    return vec_feats        \n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "def create_next_prev_vector_features(x,model):\n",
    "    ndim = model.vector_size\n",
    "    index2word_set = set( model.index2word )\n",
    "    words = x.lower().split(\" \")\n",
    "    previous_vector_feats = []\n",
    "    second_previous_vector_feats = []\n",
    "    next_vector_feats = []\n",
    "    second_next_vector_feats = []\n",
    "    for i in range(0,len(words)):\n",
    "        #check if previous token is in model\n",
    "                if (i-1) >= 0:\n",
    "                    if (words[i-1].lower() in index2word_set):\n",
    "                               previous_vector_feats.append(model[words[i-1]])\n",
    "                    else:\n",
    "                                previous_vector_feats.append(np.zeros(ndim))\n",
    "                else:\n",
    "                    previous_vector_feats.append(np.zeros(ndim))\n",
    "                \n",
    "                \n",
    "                 #check if second previous token is in model\n",
    "                if (i-2) >= 0:\n",
    "                    if (words[i-2].lower() in index2word_set):\n",
    "                               second_previous_vector_feats.append(model[words[i-2]])\n",
    "                    else:\n",
    "                                second_previous_vector_feats.append(np.zeros(ndim))\n",
    "                else:\n",
    "                       second_previous_vector_feats.append(np.zeros(ndim))\n",
    "                        \n",
    "               \n",
    "                #append next vector\n",
    "                if (i+1) < len(words):\n",
    "                    if (words[i+1].lower() in index2word_set):\n",
    "                                   next_vector_feats.append(model[words[i+1]])\n",
    "                    else:\n",
    "                                 next_vector_feats.append(np.zeros(ndim))\n",
    "                else:\n",
    "                        next_vector_feats.append(np.zeros(ndim))\n",
    "                        \n",
    "                \n",
    "                 #append next vector\n",
    "                if (i+2) < len(words):\n",
    "                    if (words[i+2].lower() in index2word_set):\n",
    "                                   second_next_vector_feats.append(model[words[i+2]])\n",
    "                    else:\n",
    "                                second_next_vector_feats.append(np.zeros(ndim))\n",
    "                else:\n",
    "                        second_next_vector_feats.append(np.zeros(ndim))\n",
    "    \n",
    "    \n",
    "    return previous_vector_feats,next_vector_feats,second_next_vector_feats,second_previous_vector_feats\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "def create_morph_feats(x):\n",
    "    #morphological features\n",
    "        words = nltk.word_tokenize(x)\n",
    "       \n",
    "        sent_morph_feats =[]\n",
    "        for w in words:\n",
    "                morph_feats=[]\n",
    "                \n",
    "                if w[0].isupper(): #is first letter capital\n",
    "                    morph_feats.append(1)\n",
    "                else:\n",
    "                    morph_feats.append(0)\n",
    "\n",
    "                capitals = 0\n",
    "                lowers = 0\n",
    "                for letter in w:\n",
    "                    if letter.isupper():\n",
    "                        capitals = capitals + 1\n",
    "                    if letter.islower():\n",
    "                        lowers = lowers + 1\n",
    "\n",
    "                if w[0].islower() and capitals > 0: #contains capitals, except 1st letter\n",
    "                    morph_feats.append(1)\n",
    "                else:\n",
    "                    morph_feats.append(0)\n",
    "\n",
    "                if capitals == len(w): #is all letters capitals\n",
    "                    morph_feats.append(1)\n",
    "                else:\n",
    "                    morph_feats.append(0)\n",
    "\n",
    "                if lowers == len(w): #is all letters lower\n",
    "                    morph_feats.append(1)\n",
    "                else:\n",
    "                    morph_feats.append(0)\n",
    "\n",
    "                if len(re.findall(r\"\\d\", w)) == len(w): #is all letters digits\n",
    "                    morph_feats.append(1)\n",
    "                else:\n",
    "                    morph_feats.append(0)\n",
    "\n",
    "                if len(re.findall(r\"[a-zA-Z]\", w)) == len(w): #is all letters words\n",
    "                    morph_feats.append(1)\n",
    "                else:\n",
    "                    morph_feats.append(0)\n",
    "\n",
    "                \n",
    "                \n",
    "                sent_morph_feats.append(morph_feats)\n",
    "        return sent_morph_feats\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "def create_pos_feats(x,pos_lexicon):\n",
    "    words = nltk.word_tokenize(x)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    tags_list = [] #the pos list\n",
    "    pos_sent_feats = []\n",
    "    for _, t in tags:\n",
    "                tags_list.append(t)\n",
    "    i =0        \n",
    "    for w in words:\n",
    "            pos_feats = []\n",
    "            for p in pos_lexicon:\n",
    "\n",
    "                            #check the POS tag of the current word\n",
    "                            if tags_list[i] == p:\n",
    "                                pos_feats.append(1)\n",
    "                            else:\n",
    "                                pos_feats.append(0)\n",
    "            pos_sent_feats.append(pos_feats)\n",
    "            i = i+1\n",
    "    return pos_sent_feats\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "def create_prev_pos_feats(x,pos_lexicon):\n",
    "    words = nltk.word_tokenize(x)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    tags_list = [] #the pos list\n",
    "    pos_sent_prev_feats = []\n",
    "    pos_sent_next_feats = []\n",
    "    pos_sent_second_prev_feats = []\n",
    "    pos_sent_second_next_feats = []\n",
    "    \n",
    "    for _, t in tags:\n",
    "                tags_list.append(t)\n",
    "    i =0        \n",
    "    for w in words:\n",
    "            previous_pos_feats = []\n",
    "            second_previous_pos_feats = []\n",
    "            next_pos_feats = []\n",
    "            second_next_pos_feats = []\n",
    "            for p in pos_lexicon:\n",
    "                    \n",
    "                    #check the POS tag of the previous word (if the index is IN list's bounds)\n",
    "                    if (i-1) >= 0:\n",
    "                        if tags_list[i-1] == p:\n",
    "                            previous_pos_feats.append(1)\n",
    "                        else:\n",
    "                            previous_pos_feats.append(0)\n",
    "                    else:\n",
    "                        previous_pos_feats.append(0)\n",
    "                            \n",
    "                    #check the POS tag of the 2nd previous word (if the index is IN list's bounds)\n",
    "                    if (i-2) >= 0:\n",
    "                        if tags_list[i-2] == p:\n",
    "                            second_previous_pos_feats.append(1)\n",
    "                        else:\n",
    "                            second_previous_pos_feats.append(0)\n",
    "                    else:\n",
    "                        second_previous_pos_feats.append(0)\n",
    "                            \n",
    "                    #check the POS tag of the next word (if the index is IN list's bounds)\n",
    "                    if (i+1) < len(words):\n",
    "                        if tags_list[i+1] == p:\n",
    "                            next_pos_feats.append(1)\n",
    "                        else:\n",
    "                            next_pos_feats.append(0)\n",
    "                    else:\n",
    "                        next_pos_feats.append(0)\n",
    "                            \n",
    "                    #check the POS tag of the next word (if the index is IN list's bounds)\n",
    "                    if (i+2) < len(words):\n",
    "                        if tags_list[i+2] == p:\n",
    "                            second_next_pos_feats.append(1)\n",
    "                        else:\n",
    "                            second_next_pos_feats.append(0)\n",
    "                    else:\n",
    "                        second_next_pos_feats.append(0)\n",
    "            \n",
    "            \n",
    "            pos_sent_prev_feats.append(previous_pos_feats)\n",
    "            pos_sent_next_feats.append(next_pos_feats)\n",
    "            pos_sent_second_prev_feats.append(second_previous_pos_feats)\n",
    "            pos_sent_second_next_feats.append(second_next_pos_feats)\n",
    "            i = i+1\n",
    "    return pos_sent_prev_feats,pos_sent_next_feats,pos_sent_second_prev_feats,pos_sent_second_next_feats\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "def predict_aspect_terms(cleanText,test_sentences):\n",
    "   \n",
    "    predictedLabels = ssvm.predict(test_sentences)\n",
    "    # aspect terms\n",
    "    words = cleanText.split(\" \")\n",
    "    start_aspectTerm = False\n",
    "    ote=''\n",
    "    predictedLabels = predictedLabels[0]\n",
    "    aspect_term_predicted = []\n",
    "    for i in range(0,len(predictedLabels)):\n",
    "        \n",
    "        if predictedLabels[i] == 1 and start_aspectTerm == False:\n",
    "            start_aspectTerm = True\n",
    "            ote = words[i]\n",
    "            if(i == len(predictedLabels) - 1):\n",
    "                aspect_term_predicted.append(ote.lower())\n",
    "        elif  predictedLabels[i] == 1 and start_aspectTerm == True:\n",
    "            aspect_term_predicted.append(ote.lower())\n",
    "            ote = words[i]\n",
    "            if(i == len(predictedLabels) - 1):\n",
    "                aspect_term_predicted.append(ote)\n",
    "        elif predictedLabels[i] == 2 and start_aspectTerm == True:\n",
    "            ote = ote + ' '+words[i]\n",
    "            if(i == len(predictedLabels) - 1):\n",
    "                aspect_term_predicted.append(ote.lower())\n",
    "        elif predictedLabels[i] == 0 and start_aspectTerm == True:\n",
    "            start_aspectTerm = False\n",
    "            aspect_term_predicted.append(ote.lower())\n",
    "    return aspect_term_predicted\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def predictOTE(text):\n",
    "    #load model\n",
    "    text = text.strip()\n",
    "    #clean text\n",
    "    cleanText = review_to_words_ote(text)\n",
    "   \n",
    "    #create vector features\n",
    "    vec = create_vector_features(cleanText,model)\n",
    "   \n",
    "    \n",
    "    # create vector features of next previous words\n",
    "    #previous_vector_feats,next_vector_feats = create_next_prev_vector_features(cleanText,model,index2word_set)\n",
    "    previous_vector_feats,next_vector_feats,second_next_vector_feats,second_previous_vector_feats = create_next_prev_vector_features(cleanText,model)\n",
    "    #create morph features\n",
    "    morph_feats = create_morph_feats(cleanText)\n",
    "    \n",
    "    # create POS features\n",
    "    pos_sent_feats = create_pos_feats(cleanText,pos_lexicon)\n",
    "   \n",
    "    # create previous ,next word POS features\n",
    "    #pos_sent_prev_feats,pos_sent_next_feats = create_prev_pos_feats(cleanText)\n",
    "    pos_sent_prev_feats,pos_sent_next_feats,pos_sent_second_prev_feats,pos_sent_second_next_feats = create_prev_pos_feats(cleanText,pos_lexicon)\n",
    "    \t\n",
    "    \n",
    "    #prepare a array of features\n",
    "    test_sentences = []\n",
    "    x = cleanText.split(\" \")\n",
    "    test_word_feats = []\n",
    "    for i,word in enumerate(x):\n",
    "        test_word_features = []\n",
    "        test_word_features.append(vec[i])\n",
    "        test_word_features.append(morph_feats[i])\n",
    "        test_word_features.append(pos_sent_feats[i])\n",
    "        \n",
    "        \n",
    "        test_word_features.append(pos_sent_prev_feats[i])\n",
    "        test_word_features.append(pos_sent_next_feats[i])\n",
    "        test_word_features.append(pos_sent_second_prev_feats[i])\n",
    "        test_word_features.append(pos_sent_second_next_feats[i])\n",
    "        \n",
    "        test_word_features.append(previous_vector_feats[i])\n",
    "        test_word_features.append(next_vector_feats[i])\n",
    "        test_word_features.append(second_previous_vector_feats[i])\n",
    "        test_word_features.append(second_next_vector_feats[i])\n",
    "        \n",
    "        test_word_feats.append( np.concatenate((test_word_features[0],test_word_features[1],test_word_features[2],test_word_features[3],test_word_features[4],\n",
    "                              test_word_features[5],test_word_features[6],test_word_features[7],test_word_features[8],test_word_features[9],test_word_features[10]),axis = 0))\n",
    "        \n",
    "   \n",
    "    test_sentences_array = np.zeros((len(test_word_feats), len(test_word_feats[0])))\n",
    "    index_i = 0\n",
    "    for index_i in range(0,len(test_word_feats)):\n",
    "        for index_j in range(0,len(test_word_feats[0])):\n",
    "                test_sentences_array[index_i, index_j] = test_word_feats[index_i][index_j]\n",
    "\n",
    "    test_sentences.append(test_sentences_array)\n",
    "   \n",
    "    aspect_term_predicted = predict_aspect_terms(cleanText,test_sentences)\n",
    "    \n",
    "    #post processing\n",
    "    # assuming that  the length of aspect term <= 3\n",
    "    rem_terms = []\n",
    "    for term in aspect_term_predicted:\n",
    "            \n",
    "        if(len(term.split(' ')) % 2 == 0 and len(term.split(' ')) > 3 ):\n",
    "            rem_terms.append(term) #terms to be removed later\n",
    "            x = term.split(' ')\n",
    "            for i in range(0,len(x),2):\n",
    "                aspect_term_predicted.append(x[i] + ' ' + x[i+1])\n",
    "                \n",
    "    for term in rem_terms:\n",
    "            aspect_term_predicted.remove(term)\n",
    "\n",
    "    return aspect_term_predicted\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "nan_words = {}\n",
    "\n",
    "def makeFeatureVec( words, model, num_features, index2word_set ):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            if np.isnan( model[ word ] ).any():\n",
    "                if word in nan_words:\n",
    "                    nan_words[ word ] += 1\n",
    "                else:\n",
    "                    nan_words[ word ] = 1\n",
    "    \n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    if nwords != 0:\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features, index2word_set ):\n",
    "    counter = 0.\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "            reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features, index2word_set )\n",
    "            counter = counter + 1.\n",
    "\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "def computeCategorySimilarity(x,model,index2word_set):\n",
    "    \n",
    "    # cat_words_vectors\n",
    "    cat_words_vectors = []\n",
    "    # Need to make it dynamic\n",
    "    for cat in categories:\n",
    "                similar_words_vec = []\n",
    "                similar_words = model.most_similar(cat)\n",
    "                for word in similar_words:\n",
    "                    similar_words_vec.append(model[word[0]])\n",
    "                cat_words_vectors.append(similar_words_vec)\n",
    "                \n",
    "                \n",
    "    words = x.split(\" \")\n",
    "    words_vectors = []\n",
    "    similarity_from_category = np.zeros((len(cat_words_vectors),),dtype=\"float32\")\n",
    "    if(x != ''):\n",
    "        for w in words:\n",
    "            if w in index2word_set:\n",
    "                words_vectors.append(model[w])\n",
    "                \n",
    "        #computing word vector similarity from each category \n",
    "        if(len(words_vectors) > 0):\n",
    "               for cat in cat_words_vectors:\n",
    "                    np.append(similarity_from_category,cosine_similarity(words_vectors,cat).max())\n",
    "        else:\n",
    "            similarity_from_category =np.zeros(len(cat_words_vectors))\n",
    "    else:\n",
    "        similarity_from_category= np.zeros(len(cat_words_vectors))\n",
    "    return similarity_from_category\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "def return_prob_positive_class(ans):\n",
    "    result = []\n",
    "    for i in range(0,len(ans)):\n",
    "        result.append(ans[i][1])\n",
    "    return result\n",
    "def predict_cat_labels(test_SF,testfeats):\n",
    "    for c in range(0,len(categories)):\n",
    "        colname = str(categories[c])\n",
    "        test_SF[colname] = return_prob_positive_class(models_acd[c].predict_proba(testfeats))\n",
    "    colnames = categories #['restaurant','price','misc','food','beverages','ambiance','service','location']\n",
    "    row = test_SF.head(1)\n",
    "    r_labels =[]\n",
    "    for c in colnames:\n",
    "            for index,row in test_SF.iterrows():\n",
    "                if(row[c] >= 0.5):\n",
    "                        r_labels.append(c)\n",
    "        \n",
    "    return r_labels\n",
    "\n",
    "def predict_cat_labels_bulk(test_SF,testfeats):\n",
    "    for c in range(0,len(categories)):\n",
    "        colname = str(categories[c])\n",
    "        test_SF[colname] = return_prob_positive_class(models_acd[c].predict_proba(testfeats))\n",
    "   \n",
    "    colnames = categories #['restaurant','price','misc','food','beverages','ambiance','service','location']\n",
    "    f_pred = []\n",
    "    for index,row in test_SF.iterrows():\n",
    "        r_labels =[]\n",
    "        for c in colnames:\n",
    "                if(row[c] >= 0.5):\n",
    "                        r_labels.append(c)\n",
    "        f_pred.append(r_labels)\n",
    "    return f_pred\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "def return_category_vec(x):\n",
    "    return model[x]\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "def find_near_words(aspect_term,text,window):\n",
    "    if(aspect_term != 'null'):\n",
    "        aspect_term = re.sub('[\\',()]','',aspect_term)\n",
    "        aspect_term = re.sub(\"[^a-zA-Z]\", \" \",aspect_term).lower()\n",
    "        \n",
    "        multi_asp = aspect_term.split(' ')\n",
    "        aspect_term = ''\n",
    "        for w in multi_asp:\n",
    "            if(len(w) != 0):\n",
    "                if w not in stopwords:\n",
    "                    aspect_term = aspect_term+' '+w\n",
    "        #text = re.sub(\"[^a-zA-Z]\", \" \",text).lower()\n",
    "        aspect_term = aspect_term.strip()\n",
    "        new_text = text.replace(aspect_term,aspect_term.replace(' ',''))\n",
    "        new_aspect_term = aspect_term.replace(' ','')\n",
    "        left_words = ''\n",
    "        right_words =''\n",
    "        sent = new_text.split(' ')\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if sent[i].find(new_aspect_term) != -1:\n",
    "                index_at = i\n",
    "                break\n",
    "        #index_at = sent.index(new_aspect_term)\n",
    "        if(index_at >= 0):\n",
    "            j = index_at - window\n",
    "            for count in range(0,window):\n",
    "                if(j >= 0):\n",
    "                    left_words = left_words + ' ' + sent[j]\n",
    "                j = j + 1\n",
    "\n",
    "            j = index_at + 1\n",
    "            for count in range(0,window):\n",
    "                if(j < len(sent)):\n",
    "                    right_words = right_words + ' ' + sent[j]\n",
    "                j = j + 1\n",
    "\n",
    "            phrase = left_words + ' ' +aspect_term + right_words\n",
    "        else:\n",
    "            phrase = text\n",
    "    else:\n",
    "        phrase = text\n",
    "        \n",
    "    return phrase\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "def compute_label(pos_prob,neg_prob,neu_prob):\n",
    "    if(pos_prob < 0.2 and neu_prob < 0.2 and neg_prob < 0.2):\n",
    "        return \"positive\"\n",
    "    \n",
    "    elif(abs(neu_prob - neg_prob) < 0.2 and pos_prob < 0.2):\n",
    "        return \"neutral\"\n",
    "    \n",
    "    elif(abs(neu_prob - pos_prob) < 0.2 and neg_prob < 0.2):\n",
    "        return \"neutral\"\n",
    "    \n",
    "    elif(neu_prob > pos_prob and neu_prob > neg_prob):\n",
    "        return \"neutral\"\n",
    "    \n",
    "    elif( pos_prob > neu_prob  and abs(neg_prob - pos_prob) > 0.2 and pos_prob > neg_prob):\n",
    "        return \"positive\"\n",
    "    elif(neg_prob > pos_prob  and abs(neg_prob - pos_prob) > 0.2 and neg_prob > neu_prob):\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "def predict_polarity(test_pd,testfeats):\n",
    "    for p in range(0,len(polarities)):\n",
    "        cname = polarities[p] + '_prob'\n",
    "        test_pd[cname] = return_prob_positive_class(models_pd[p].predict_proba(testfeats))\n",
    "   \n",
    "    labels = []\n",
    "    if('neutral_prob' in test_pd.columns):\n",
    "            for index,rev in test_pd.iterrows():\n",
    "                labels.append(compute_label(rev['positive_prob'],rev['negative_prob'],rev['neutral_prob']))\n",
    "    else:\n",
    "            for index,rev in test_pd.iterrows():\n",
    "                labels.append(compute_label(rev['positive_prob'],rev['negative_prob'],0))\n",
    "\n",
    "    test_pd['predictedLabels_multi'] = labels\n",
    "    return test_pd\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "def compute_polarity(polarity_sf):\n",
    "    # category_vec feature\n",
    "    cat_vec = []\n",
    "    \n",
    "    cat_vec_test = []\n",
    "    for index,rev in polarity_sf.iterrows():\n",
    "        cat_vec_test.append(return_category_vec(rev['modifiedcategory']))\n",
    "  \n",
    "    clean_test_reviews = []\n",
    "    for review in polarity_sf['text']:\n",
    "        clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ) )\n",
    "\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, ndim, index2word_set )\n",
    "    #find near words window 5,2\n",
    "    \n",
    "    near_words_5 = []\n",
    "    near_words_2 = []\n",
    "    for index,rev in polarity_sf.iterrows():\n",
    "        near_words_5.append(find_near_words(rev['aspectterm'],rev['cleanText'],5))\n",
    "        near_words_2.append(find_near_words(rev['aspectterm'],rev['cleanText'],2))\n",
    "    \n",
    "    polarity_sf['nearwords_window5'] = near_words_5\n",
    "    polarity_sf['nearwords_window2'] = near_words_2\n",
    "             \n",
    "    \n",
    "    \n",
    "    #compute lexicon feats\n",
    "    pos_feats1 = []\n",
    "    pos_feats2 = []\n",
    "   \n",
    "    for index,rev in polarity_sf.iterrows():\n",
    "        pos_feats1.append(create_lexicon_feats(rev['nearwords_window2'],yelp_lex,yelp_scores))\n",
    "        pos_feats2.append(create_lexicon_feats(rev['nearwords_window5'],yelp_lex,yelp_scores))\n",
    "        \n",
    "    \n",
    "    polarity_sf['yelp_lex_feats_nearwords_window2'] = pos_feats1\n",
    "    polarity_sf['yelp_lex_feats_nearwords_window5'] = pos_feats2\n",
    "  \n",
    "    \n",
    "    test_feats =np.column_stack((testDataVecs,polarity_sf.yelp_lex_feats_nearwords_window2,\n",
    "                             polarity_sf.yelp_lex_feats_nearwords_window5,                           \n",
    "                             cat_vec_test))\n",
    "\n",
    "   \n",
    "    del near_words_5,near_words_2,pos_feats1,pos_feats2\n",
    "    predictions = predict_polarity(polarity_sf,test_feats)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "def assign_aspectterm_category(aspect_terms,categories_pred):\n",
    "    #aspect terms : predicted from ote module\n",
    "    #categories:predicted from aspect category detection system\n",
    "    \n",
    "    if len(aspect_terms) > 0:\n",
    "            aspect_term_vec = []\n",
    "            category_vec = []\n",
    "            ans = []\n",
    "            for cat in categories_pred:\n",
    "                category_vec.append(return_category_vec(cat))\n",
    "\n",
    "\n",
    "            for term in aspect_terms:\n",
    "                similarity = []\n",
    "                aspect_term_vec = []\n",
    "                # dealing with multiword aspect term\n",
    "                terms = term.split(' ')\n",
    "                if len(terms) > 1:\n",
    "                    multi_aspectterm_vec = []\n",
    "                    for t in terms:\n",
    "                        if t.lower() in index2word_set:\n",
    "                            multi_aspectterm_vec.append(model[t.lower()])\n",
    "                    if(len(multi_aspectterm_vec) > 0):        \n",
    "                        for cat in category_vec:\n",
    "                            temp = cosine_similarity(multi_aspectterm_vec,cat)\n",
    "                            similarity.append(max(temp))\n",
    "\n",
    "                # for single term aspect\n",
    "                else:\n",
    "                    if term.lower() in index2word_set:\n",
    "                        aspect_term_vec.append(model[term.lower()])\n",
    "\n",
    "                    if(len(aspect_term_vec) > 0):\n",
    "                        for cat in category_vec:\n",
    "                            similarity.append(cosine_similarity(aspect_term_vec,cat))\n",
    "\n",
    "                if(len(similarity) > 0):\n",
    "                    ans.append(categories_pred[np.argmax(similarity)])\n",
    "                else:\n",
    "                    ans.append([''])\n",
    "\n",
    "\n",
    "\n",
    "            # ans are the assigned categories_pred from the predicted categories_pred\n",
    "            # for those predicted categories_pred for which we have no aspect term assign 'null' as aspect term\n",
    "            if(len(categories_pred) > 0):\n",
    "                cats_with_no_aspect_terms = list(set(categories_pred) - set(ans))\n",
    "                for cat in cats_with_no_aspect_terms:\n",
    "                    aspect_terms.append('null')\n",
    "                    ans.append(cat)\n",
    "                return aspect_terms,ans\n",
    "            \n",
    "            else:\n",
    "                return [],[]\n",
    "    \n",
    "    # case below deals for the situation when there is now aspect terms in the sentence\n",
    "    else:\n",
    "        aspect_terms = []\n",
    "        for cat in categories_pred:\n",
    "            aspect_terms.append('null')\n",
    "        return aspect_terms,categories_pred    \n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "def predict_category(text):\n",
    "    aspect_terms = predictOTE(text)\n",
    "    ss = []\n",
    "    ss.append(text)\n",
    "    test_sf = pd.DataFrame({'text' : ss})\n",
    "    test_sf['cleanText'] = test_sf['text'].apply(review_to_words)\n",
    "    clean_test_reviews = []\n",
    "    for review in test_sf['text']:\n",
    "        clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ) )\n",
    "\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, ndim, index2word_set )\n",
    "    #test_sf['vectors'] = testDataVecs\n",
    "    cs = np.zeros((len(test_sf),len(categories)),dtype=\"float32\")\n",
    "    i = 0\n",
    "    for rev in test_sf['cleanText']:\n",
    "        cs[i] =  computeCategorySimilarity(rev,model,index2word_set)\n",
    "        i = i+1\n",
    "    \n",
    "    #test_sf['cs'] = cs\n",
    "    test_feats = np.column_stack((testDataVecs,cs))\n",
    "    predictedLabels = predict_cat_labels(test_sf,test_feats)\n",
    "    aspect_t,cats = assign_aspectterm_category(aspect_terms,predictedLabels)\n",
    "    \n",
    "    # now predict sentiment of each category\n",
    "    sent = []\n",
    "    text_array = []\n",
    "    cleanText_array = []\n",
    "    for label in cats :\n",
    "        #sent.append(test_sf['vectors'][0])\n",
    "        text_array.append(test_sf['text'][0])\n",
    "        cleanText_array.append(test_sf['cleanText'][0])\n",
    "   \n",
    "    polarity_sf = pd.DataFrame({'text' : text_array,'aspectterm' : aspect_t,'cleanText':cleanText_array,\n",
    "                             'modifiedcategory' : cats})\n",
    "    \n",
    "    \n",
    "    if(len(polarity_sf) > 0):\n",
    "        polarity_sf = compute_polarity(polarity_sf)\n",
    "        return polarity_sf[['text','aspectterm','modifiedcategory','predictedLabels_multi']]\n",
    "        #return polarity_sf\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "def predict_category_review(text):\n",
    "    text = text.strip()\n",
    "    text_array = tokenize.sent_tokenize(text)\n",
    "    text = []\n",
    "    cat = []\n",
    "    at = []\n",
    "    pol = []\n",
    "    for tex in text_array:\n",
    "        if(tex != ''):\n",
    "                #aspect_term_predicted = predict_ote.predictOTE(tex)\n",
    "                #cat_labels = PolarityDetection.predict_category(tex,aspect_term_predicted)\n",
    "                cat_labels = predict_category(tex)\n",
    "                if len(cat_labels) > 0:\n",
    "                    for index,row in cat_labels.iterrows():\n",
    "                        text.append(row['text'])\n",
    "                        cat.append(row['modifiedcategory'])\n",
    "                        at.append(row['aspectterm'])\n",
    "                        pol.append(row['predictedLabels_multi'])\n",
    "                    \n",
    "    j = pd.DataFrame({'text' : text,'category' :cat,'aspectterm' :at,'polarity' :pol})\n",
    "    \n",
    "    #summary of overall review\n",
    "    s = j[['category','polarity']]\n",
    "    #summary of overall review\n",
    "    s['count'] = s.groupby(['category','polarity']).polarity.transform('count')\n",
    "    s = s.sort_values('polarity',ascending=False)\n",
    "    idx = s.groupby(['category'])['count'].idxmax()\n",
    "    s = s.loc[idx, ['category', 'polarity']]\n",
    "\n",
    "    return j,s\n",
    "\n",
    "\n",
    "\n",
    "# In[268]:\n",
    "\n",
    "#bulk review analysis\n",
    "def bulk_reviews(reviews):\n",
    "    \n",
    "    reviews_array = reviews.strip().split('\\n')\n",
    "    r_id = 0\n",
    "    r_id_array = []\n",
    "    sent = []\n",
    "    for rev in reviews_array:\n",
    "        if(rev != ''):\n",
    "            sentences = nltk.sent_tokenize(rev)\n",
    "            for s in sentences:\n",
    "                r_id_array.append(r_id)\n",
    "                sent.append(s)\n",
    "            r_id = r_id + 1\n",
    "    \n",
    "    reviews = pd.DataFrame({'review_id' : r_id_array,'text' : sent})\n",
    "    reviews['cleanText'] = reviews['text'].apply(review_to_words)\n",
    "    ote = []\n",
    "    for index,rev in reviews.iterrows():\n",
    "        ote.append(predictOTE(rev['text']))\n",
    "    reviews['aspect terms'] = ote\n",
    "    clean_test_reviews = []\n",
    "    for review in reviews['text']:\n",
    "        clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ) )\n",
    "\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, ndim, index2word_set )\n",
    "    #reviews['vectors'] = testDataVecs\n",
    "    cs = np.zeros((len(reviews),len(categories)),dtype=\"float32\")\n",
    "    i = 0\n",
    "    for rev in reviews['cleanText']:\n",
    "        cs[i] = computeCategorySimilarity(rev,model,index2word_set)\n",
    "        i = i+1\n",
    "    \n",
    "    #reviews['cs'] = cs\n",
    "    test_feats = np.column_stack((testDataVecs,cs))\n",
    "   \n",
    "    reviews['predictedLabels'] = predict_cat_labels_bulk(reviews,test_feats)\n",
    "    \n",
    "    ats = []\n",
    "    cts = []\n",
    "    pol = []\n",
    "    for index,rev in reviews.iterrows():\n",
    "        aspect_t,cats = assign_aspectterm_category(rev['aspect terms'],rev['predictedLabels'])\n",
    "    \n",
    "        # now predict sentiment of each category\n",
    "        sent = []\n",
    "        text_array = []\n",
    "        cleanText_array = []\n",
    "        for label in cats :\n",
    "            #sent.append(rev['vectors'])\n",
    "            text_array.append(rev['text'])\n",
    "            cleanText_array.append(rev['cleanText'])\n",
    "   \n",
    "        polarity_sf = pd.DataFrame({'text' : text_array,'aspectterm' : aspect_t,'cleanText':cleanText_array,\n",
    "                                 'modifiedcategory' : cats})\n",
    "    \n",
    "        if(len(polarity_sf) > 0):\n",
    "            polarity_sf = compute_polarity(polarity_sf)\n",
    "            pol.append(polarity_sf['predictedLabels_multi'])\n",
    "            ats.append(aspect_t)\n",
    "            cts.append(cats)\n",
    "        else:\n",
    "            pol.append([])\n",
    "            ats.append([])\n",
    "            cts.append([])\n",
    "    reviews['category'] = cts\n",
    "    reviews['aspect term'] = ats\n",
    "    reviews['polarity'] = pol\n",
    "    \n",
    "    ats = []\n",
    "    cts = []\n",
    "    pol = []\n",
    "    txt = []\n",
    "    rid = []\n",
    "    for index,rev in reviews.iterrows():\n",
    "        for i in range(0,len(rev['aspect term'])):\n",
    "            ats.append(rev['aspect term'][i])\n",
    "            cts.append(rev['category'][i])\n",
    "            pol.append(rev['polarity'][i])\n",
    "            txt.append(rev['text'])\n",
    "            rid.append(rev['review_id'])\n",
    "            \n",
    "    finalsf = pd.DataFrame({'review_id' : rid,'text':txt,'category' : cts,'aspect term' : ats,'polarity' : pol}) \n",
    "    \n",
    "    #return category wise summary of sentiments\n",
    "    s = finalsf[['review_id','category','polarity','aspect term']]\n",
    "    \n",
    "    s['count'] = s.groupby(['review_id','category','polarity']).polarity.transform('count')\n",
    "    s = s.sort_values('polarity',ascending=False)\n",
    "    idx = s.groupby(['review_id','category'])['count'].idxmax()\n",
    "    s = s.loc[idx, ['review_id','category', 'polarity']]\n",
    "    f = s.groupby(['category','polarity'])['polarity'].agg({'count':'count'})\n",
    "    \n",
    "    \n",
    "    pos_c = []\n",
    "    neg_c = []\n",
    "    neu_c = []\n",
    "    cate = f.index.levels[0]\n",
    "    for c in cate:\n",
    "            pos = 0\n",
    "            neg= 0\n",
    "            neu = 0\n",
    "            for index,rows in f.iterrows():\n",
    "                    if(index[0] == c and index[1] == 'positive'):\n",
    "                        pos = rows['count']\n",
    "                    if(index[0] == c and index[1] == 'negative'):\n",
    "                        neg = rows['count']    \n",
    "                    if(index[0] == c and index[1] == 'neutral'):\n",
    "                        neu = rows['count'] \n",
    "            pos_c.append(pos)\n",
    "            neg_c.append(neg)\n",
    "            neu_c.append(neu)\n",
    "\n",
    "    new_cat_wise = pd.DataFrame({'category' : cate,'sentiment.positive' : pos_c,'sentiment.negative' : neg_c,'sentiment.neutral' : neu_c})\n",
    "    \n",
    "    #producing aspect wise sentiment summary\n",
    "    del s\n",
    "    s = finalsf[['review_id','category','polarity','aspect term']]\n",
    "    s['count'] = s.groupby(['review_id','aspect term','polarity']).polarity.transform('count')\n",
    "    s = s.sort_values('polarity',ascending=False)\n",
    "    \n",
    "    idx = s.groupby(['review_id','aspect term'])['count'].idxmax()\n",
    "    s = s.loc[idx, ['review_id','aspect term', 'polarity']]\n",
    "    f = s.groupby(['aspect term','polarity'])['polarity'].agg({'count':'count'})\n",
    "    \n",
    "    \n",
    "    pos_c = []\n",
    "    neg_c = []\n",
    "    neu_c = []\n",
    "    cate = f.index.levels[0]\n",
    "    for c in cate:\n",
    "            pos = 0\n",
    "            neg= 0\n",
    "            neu = 0\n",
    "            for index,rows in f.iterrows():\n",
    "                    if(index[0] == c and index[1] == 'positive'):\n",
    "                        pos = rows['count']\n",
    "                    if(index[0] == c and index[1] == 'negative'):\n",
    "                        neg = rows['count']    \n",
    "                    if(index[0] == c and index[1] == 'neutral'):\n",
    "                        neu = rows['count'] \n",
    "            pos_c.append(pos)\n",
    "            neg_c.append(neg)\n",
    "            neu_c.append(neu)\n",
    "\n",
    "    new_term_wise = pd.DataFrame({'aspect term' : cate,'sentiment.positive' : pos_c,'sentiment.negative' : neg_c,'sentiment.neutral' : neu_c})\n",
    "    new_term_wise['count'] = new_term_wise['sentiment.positive'] + new_term_wise['sentiment.neutral'] + new_term_wise['sentiment.negative']\n",
    "    new_term_wise = new_term_wise[new_term_wise['aspect term'] != 'null']\n",
    "    \n",
    "    return new_cat_wise,new_term_wise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
