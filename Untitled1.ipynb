{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "def recreate_df(train_pd):\n",
    "    text = []\n",
    "    ate = []\n",
    "    cat = []\n",
    "    pol = []\n",
    "    \n",
    "    for i,row in train_pd.iterrows():\n",
    "        aspect_terms = row['aspect term'].split(';')\n",
    "        aspect_terms = filter(None,aspect_terms)\n",
    "        categories = row['category'].split(' ')\n",
    "        categories = filter(None,categories)\n",
    "        polarity = row['polarity'].split(' ')\n",
    "        polarity = filter(None,polarity)\n",
    "        if(len(polarity) == len(categories) and len(categories) == len(aspect_terms) ):\n",
    "            for j in range(0,len(aspect_terms)):\n",
    "                text.append(row['text'])\n",
    "                ate.append(aspect_terms[j])\n",
    "                cat.append(categories[j])\n",
    "                pol.append(polarity[j])\n",
    "    \n",
    "    new_train = pd.DataFrame({'text' : text,'aspect term' : ate,'category' : cat,'polarity' :pol })\n",
    "    return new_train            \n",
    "    \n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "def load_polarity_lexicon(filename):\n",
    "    lex = []\n",
    "    scores=[]\n",
    "    f = open(filename, \"r\")#'saif_lex/Yelp-restaurant-reviews-AFFLEX-NEGLEX-unigrams.txt'\n",
    "    for line in f:\n",
    "        l = line.split()\n",
    "        tag = l[0]\n",
    "        score = l[1]\n",
    "        if(len(re.findall('[_NEG]',tag)) == 0):\n",
    "            lex.append(tag)\n",
    "            scores.append(score)\n",
    "        \n",
    "    return lex,scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "stopwords =  nltk.corpus.stopwords.words('english')\n",
    "not_remove = ['not','very','few','more','only','nor','but','don','t','and','with','no']\n",
    "for word in not_remove:\n",
    "    stopwords.remove(word)\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "# reading stop word list from nltk\n",
    "import nltk\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    #remove non alphanumeric characters\n",
    "    letters_only = re.sub(\"\\'\", \"\",raw_review) \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",letters_only) \n",
    "    # convert into lowercase and split text into words using split() function\n",
    "    words = letters_only.lower().split()\n",
    "    # declaring empty array\n",
    "    cleanwords = []\n",
    "    for word in words:\n",
    "        if(word not in stopwords):\n",
    "            cleanwords.append(word)\n",
    "    return( \" \".join( cleanwords ))\n",
    "\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "def create_lexicon_feats(x,yelp_lex,yelp_scores):\n",
    "    x = x.lower()\n",
    "    x = re.sub('\\.',' ',x).strip()\n",
    "    words = x.split(' ')\n",
    "    count = 0\n",
    "    for j in range(0,len(words)):\n",
    "        if (words[j] not in ['don\\'t','no','dont','not','never']):\n",
    "            for i in range(0,len(yelp_lex)):\n",
    "                \n",
    "                 if words[j] == yelp_lex[i]:\n",
    "                    if(j >0):\n",
    "                        if(words[j-1] in ['don\\'t','no','dont','not','never']):\n",
    "                            count = count + (-1)*float(yelp_scores[i])\n",
    "                        else:\n",
    "                            count = count + float(yelp_scores[i])\n",
    "                    else:        \n",
    "                        count = count + float(yelp_scores[i])\n",
    "                    break\n",
    "                    \n",
    "       \n",
    "    return count\n",
    "    \n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "def find_near_words(aspect_term,text,window):\n",
    "    if(aspect_term != 'null'):\n",
    "        aspect_term = re.sub('[\\',()]','',aspect_term)\n",
    "        aspect_term = re.sub(\"[^a-zA-Z]\", \" \",aspect_term).lower()\n",
    "        \n",
    "        multi_asp = aspect_term.split(' ')\n",
    "        aspect_term = ''\n",
    "        for w in multi_asp:\n",
    "            if(len(w) != 0):\n",
    "                if w not in stopwords:\n",
    "                    aspect_term = aspect_term+' '+w\n",
    "        #text = re.sub(\"[^a-zA-Z]\", \" \",text).lower()\n",
    "        aspect_term = aspect_term.strip()\n",
    "        new_text = text.replace(aspect_term,aspect_term.replace(' ',''))\n",
    "        new_aspect_term = aspect_term.replace(' ','')\n",
    "        left_words = ''\n",
    "        right_words =''\n",
    "        sent = new_text.split(' ')\n",
    "        index_at = -1\n",
    "        for i in range(0,len(sent)):\n",
    "            if sent[i].find(new_aspect_term) != -1:\n",
    "                index_at = i\n",
    "                break\n",
    "        #index_at = sent.index(new_aspect_term)\n",
    "        if(index_at >= 0):\n",
    "            j = index_at - window\n",
    "            for count in range(0,window):\n",
    "                if(j >= 0):\n",
    "                    left_words = left_words + ' ' + sent[j]\n",
    "                j = j + 1\n",
    "\n",
    "            j = index_at + 1\n",
    "            for count in range(0,window):\n",
    "                if(j < len(sent)):\n",
    "                    right_words = right_words + ' ' + sent[j]\n",
    "                j = j + 1\n",
    "\n",
    "            phrase = left_words + ' ' +aspect_term + right_words\n",
    "        else:\n",
    "            phrase = text\n",
    "    else:\n",
    "        phrase = text\n",
    "        \n",
    "    return phrase\n",
    "    \n",
    "\n",
    "nan_words = {}\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "def return_category_vec(x,model):\n",
    "    return model[x]\n",
    "\n",
    "\n",
    "nan_words = {}\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=True ):\n",
    "    # remove HTML\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "         words = [w for w in words if not w in stopwords]\n",
    "            \n",
    "    return ( words )\n",
    "def makeFeatureVec( words, model, num_features, index2word_set ):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "           \n",
    "            if np.isnan( model[ word ] ).any():\n",
    "                if word in nan_words:\n",
    "                    nan_words[ word ] += 1\n",
    "                else:\n",
    "                    nan_words[ word ] = 1\n",
    "    \n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    if nwords != 0:\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features, index2word_set ):\n",
    "    counter = 0.\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "       if counter % 1000 == 0.:\n",
    "           print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features, index2word_set )\n",
    "       counter = counter + 1.\n",
    "\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "def create_lexicon_features_df(df,window,yelp_lex,yelp_scores):\n",
    "    pos_feats = []\n",
    "    for ind,rev in df.iterrows():\n",
    "        pos_feats.append(create_lexicon_feats(rev['nearwords_window'+window],yelp_lex,yelp_scores))\n",
    "\n",
    "    return pos_feats\n",
    "\n",
    "\n",
    "def create_binary_columns(df,polarities):\n",
    "    for polarity in polarities:\n",
    "        df['is_'+polarity] = df['polarity'] == polarity\n",
    "        df['is_'+polarity] =  df['is_'+polarity].astype(int)\n",
    "    return df\n",
    "\n",
    "# # Training\n",
    "\n",
    "# In[94]:\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "def save_model(model,directory,model_name):\n",
    "    print 'Saving model...'\n",
    "    joblib.dump(model, directory+'/'+model_name+'.pkl')\n",
    "\n",
    "def return_prob_positive_class(ans):\n",
    "    result = []\n",
    "    for i in range(0,len(ans)):\n",
    "        result.append(ans[i][1])\n",
    "    return result\n",
    "\n",
    "\n",
    "# In[108]:\n",
    "\n",
    "def train_xgboost_model_foreachPolarity(train_feats,test_feats,test_SF,polarities,train_SF):\n",
    "    x_params = { \n",
    "            'max_depth':range(3,10,2),\n",
    "            'n_estimators': [80,100],\n",
    "            'subsample':[i/100.0 for i in range(60,90,5)],\n",
    "            'min_child_weight' :range(1,6,2),\n",
    "            'colsample_bytree' :[i/100.0 for i in range(60,90,5)],\n",
    "            'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "            'learning_rate':[0.1],\n",
    "            'objective':['binary:logistic'],\n",
    "            'scale_pos_weight' : [1,5,10],\n",
    "            'seed':[5]\n",
    "           }\n",
    "    xgb_model = xgboost.XGBClassifier()\n",
    "    for pol in polarities:\n",
    "\t\t\t\tprint \"\"\n",
    "\t\t\t\tprint \"----------------------------------------\"\n",
    "\t\t\t\tprint \"Training Classifier for %s Class \" %pol\n",
    "\t\t\t\tprint \"----------------------------------------\"\n",
    "\t\t\t\tprint \"\"\n",
    "\t\t\t\tclf = RandomizedSearchCV(xgb_model, x_params,cv = 3,scoring=f1_scorer)\n",
    "\t\t\t\tclf.fit(train_feats,train_SF['is_'+pol])\n",
    "\t\t\t\t#save_model(clf.best_estimator_,'models/pd','model_'+pol)\n",
    "\t\t\t\t#print(clf.best_score_)\n",
    "\t\t\t\t#print(clf.best_params_)\n",
    "\t\t\t\tprint \"Saving model ...\"\n",
    "\t\t\t\tfilename = 'models/pd/model_'+ str(pol)+'.sav'\n",
    "\t\t\t\tpickle.dump(clf.best_estimator_, open(filename, 'wb'))\n",
    "\t\t\t\tpredict_col = 'predicted_prob_'+pol\n",
    "\t\t\t\ttest_SF[predict_col] = return_prob_positive_class(clf.predict_proba(test_feats))\n",
    "\n",
    "\t\t\t\tprint '-------------------------------------'\n",
    "\t\t\t\tprint \" F- Score : %f\"%clf.score(test_feats,test_SF['is_'+pol])\n",
    "\t\t\t\tprint \" Accuracy : %f\"%accuracy_score(test_SF['is_'+pol],clf.predict(test_feats))\n",
    "    return test_SF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_label(pos_prob,neg_prob,neu_prob = 0):\n",
    "    \n",
    "    if(pos_prob < 0.2 and neu_prob < 0.2 and neg_prob < 0.2):\n",
    "        return \"positive\"\n",
    "    \n",
    "    elif(abs(neu_prob - neg_prob) < 0.2 and pos_prob < 0.2):\n",
    "        return \"neutral\"\n",
    "    \n",
    "    elif(abs(neu_prob - pos_prob) < 0.2 and neg_prob < 0.2):\n",
    "        return \"neutral\"\n",
    "    \n",
    "    elif(neu_prob > pos_prob and neu_prob > neg_prob):\n",
    "        return \"neutral\"\n",
    "    \n",
    "    elif( pos_prob > neu_prob  and abs(neg_prob - pos_prob) > 0.2 and pos_prob > neg_prob):\n",
    "        return \"positive\"\n",
    "    elif(neg_prob > pos_prob  and abs(neg_prob - pos_prob) > 0.2 and neg_prob > neu_prob):\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('data/camera/camera_train.csv',sep='\\t')\n",
    "test_pd =  pd.read_csv('data/camera/camera_test.csv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors_filename = \"wordembeddings/amazon200.txt\"\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(vectors_filename,binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lex_file = \"lexicons/Amazon-laptops-electronics-reviews-AFFLEX-NEGLEX-unigrams.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n",
      "Extracting features...\n"
     ]
    }
   ],
   "source": [
    "ndim = model.vector_size\n",
    "index2word_set = set(model.index2word)\n",
    "\n",
    "train_pd = recreate_df(train_pd)\n",
    "test_pd = recreate_df(test_pd)\n",
    "yelp_lex,yelp_scores = load_polarity_lexicon(lex_file)\n",
    "\n",
    "print \"Cleaning text...\"\n",
    "train_pd['cleanText'] = train_pd['text'].apply(review_to_words)\n",
    "test_pd['cleanText'] = test_pd['text'].apply(review_to_words)\n",
    "\n",
    "print \"Extracting features...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n"
     ]
    }
   ],
   "source": [
    "\n",
    "near_words = []\n",
    "for ind,rev in train_pd.iterrows():\n",
    "    print ind\n",
    "    near_words.append(find_near_words(rev['aspect term'],rev['cleanText'],5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aspect term                                              picture\n",
       "category                                                 picture\n",
       "polarity                                                positive\n",
       "text           but at the same time , it takes wonderful pict...\n",
       "cleanText      but time takes wonderful pictures very easily ...\n",
       "Name: 18, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 387\n",
      "Review 0 of 100\n",
      "Extracting Lexicon Features...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_pd['nearwords_window5'] = near_words\n",
    "near_words = []\n",
    "for ind,rev in test_pd.iterrows():\n",
    "    near_words.append(find_near_words(rev['aspect term'],rev['cleanText'],5))\n",
    "test_pd['nearwords_window5'] = near_words\n",
    "near_words = []\n",
    "for ind,rev in train_pd.iterrows():\n",
    "    near_words.append(find_near_words(rev['aspect term'],rev['cleanText'],2))\n",
    "train_pd['nearwords_window2'] = near_words\n",
    "near_words = []\n",
    "for ind,rev in test_pd.iterrows():\n",
    "    near_words.append(find_near_words(rev['aspect term'],rev['cleanText'],2))\n",
    "test_pd['nearwords_window2'] = near_words\n",
    "\n",
    "\n",
    "# Extracting category vector\n",
    "cat_vec = []\n",
    "for i,rev in train_pd.iterrows():\n",
    "    cat_vec.append(return_category_vec(rev['category'],model))\n",
    "\n",
    "train_pd['category_vec'] = cat_vec\n",
    "cat_vec_test = []\n",
    "for i,rev in test_pd.iterrows():\n",
    "    cat_vec_test.append(return_category_vec(rev['category'],model))\n",
    "\n",
    "test_pd['category_vec'] = cat_vec_test\n",
    "\n",
    "\n",
    "# creating vectors\n",
    "clean_train_reviews = []\n",
    "for review in train_pd['text']:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ) )\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model,ndim, index2word_set )\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test_pd['text']:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ) )\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, ndim, index2word_set )\n",
    "\n",
    "print \"Extracting Lexicon Features...\"\n",
    "train_pd['yelp_lex_feats_nearwords_window2'] = create_lexicon_features_df(train_pd,'2',yelp_lex,yelp_scores)\n",
    "test_pd['yelp_lex_feats_nearwords_window2'] = create_lexicon_features_df(test_pd,'2',yelp_lex,yelp_scores)\n",
    "train_pd['yelp_lex_feats_nearwords_window5'] = create_lexicon_features_df(train_pd,'5',yelp_lex,yelp_scores)\n",
    "test_pd['yelp_lex_feats_nearwords_window5'] = create_lexicon_features_df(test_pd,'5',yelp_lex,yelp_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating binary Columns for each polarity....\n",
      "------- Training Sentiment Polarity Detection Classifiers --------\n",
      "\n",
      "----------------------------------------\n",
      "Training Classifier for positive Class \n",
      "----------------------------------------\n",
      "\n",
      "Saving model ...\n",
      "-------------------------------------\n",
      " F- Score : 0.932515\n",
      " Accuracy : 0.890000\n",
      "\n",
      "----------------------------------------\n",
      "Training Classifier for negative Class \n",
      "----------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zarmeen\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model ...\n",
      "-------------------------------------\n",
      " F- Score : 0.666667\n",
      " Accuracy : 0.850000\n"
     ]
    }
   ],
   "source": [
    "polarities = set(train_pd['polarity'])\n",
    "print \"Creating binary Columns for each polarity....\"\n",
    "train_pd = create_binary_columns(train_pd,polarities)\n",
    "test_pd = create_binary_columns(test_pd,polarities)\n",
    "\n",
    "train_feats =np.column_stack((trainDataVecs,train_pd.yelp_lex_feats_nearwords_window2,\n",
    "                              train_pd.yelp_lex_feats_nearwords_window5,cat_vec))\n",
    "test_feats =np.column_stack((testDataVecs,test_pd.yelp_lex_feats_nearwords_window2,\n",
    "                              test_pd.yelp_lex_feats_nearwords_window5,cat_vec_test))\n",
    "\n",
    "print \"------- Training Sentiment Polarity Detection Classifiers --------\"\n",
    "test_pd = train_xgboost_model_foreachPolarity(train_feats,test_feats,test_pd,polarities,train_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'aspect term', u'category', u'polarity', u'text', u'cleanText',\n",
       "       u'nearwords_window5', u'nearwords_window2', u'category_vec',\n",
       "       u'yelp_lex_feats_nearwords_window2',\n",
       "       u'yelp_lex_feats_nearwords_window5', u'is_positive', u'is_negative',\n",
       "       u'predicted_prob_positive', u'predicted_prob_negative'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "\t Evaluation Results \t\n",
      "--------------------------------------\n",
      "Accuracy 0.860000 :\n",
      "F Score 0.772580 :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\t # Read train and Test CSV Files\n",
    "\t#print \"Reading train and test files\"    \n",
    "\t#train_pd = pd.read_csv('data/restaurants/train.csv',sep = '\\t')\n",
    "\t#test_pd = pd.read_csv('data/restaurants/test.csv',sep = '\\t')\n",
    "\t#vectors_filename = \"vectors_yelp_200.txt\"\n",
    "\t#lex_file = 'saif_lex/Yelp-restaurant-reviews-AFFLEX-NEGLEX-unigrams.txt'\n",
    "\t#lex_file = 'saif_lex/Amazon-laptops-electronics-reviews-AFFLEX-NEGLEX-unigrams.txt'\n",
    "\t#lex_file = 'saif_lex/wnscores_inquirer.txt'\n",
    "\t#print \"Loading Word2Vec Model...\"\n",
    "\t#model = gensim.models.Word2Vec.load_word2vec_format(vectors_filename,binary=False)\n",
    "\n",
    "# ## Create Binary Columns for Polarity Classes\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "labels = []\n",
    "if 'is_neutral' in test_pd.columns:\n",
    "    for ind,rev in test_pd.iterrows():\n",
    "        labels.append(compute_label(rev['predicted_prob_positive'],rev['predicted_prob_negative'],rev['predicted_prob_neutral']))\n",
    "\n",
    "else:\n",
    "     for ind,rev in test_pd.iterrows():\n",
    "        labels.append(compute_label(rev['predicted_prob_positive'],rev['predicted_prob_negative']))\n",
    "\n",
    "test_pd['predictedLabels_multi'] = labels   \n",
    "\n",
    "acc = accuracy_score(test_pd['polarity'],test_pd['predictedLabels_multi'])\n",
    "fsco = f1_score(test_pd['polarity'],test_pd['predictedLabels_multi'],average='macro')\n",
    "\n",
    "print \"--------------------------------------\"\n",
    "print \"\\t Evaluation Results \\t\"\n",
    "print \"--------------------------------------\"\n",
    "\n",
    "print \"Accuracy %f :\"%acc\n",
    "print \"F Score %f :\"%fsco\n",
    "print \"\"\n",
    "#return acc\n",
    "    \n",
    "    \n",
    "#main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
